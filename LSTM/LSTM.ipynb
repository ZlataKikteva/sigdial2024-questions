{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f6c62f",
   "metadata": {},
   "source": [
    "## LSTM for Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, ConfusionMatrixDisplay\n",
    "from keras.layers import Dropout\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZES = [16, 32, 64]\n",
    "EPOCHS = [6, 8, 10]\n",
    "CONFIGS = [\n",
    "[\"processed_question_locution\"],\n",
    "[\"processed_response_locutions\"],\n",
    "[\"processed_preceding_locution\"],\n",
    "[\"processed_question_locution\", \"processed_response_locutions\"],\n",
    "[\"processed_question_locution\", \"processed_preceding_locution\"],\n",
    "[\"processed_preceding_locution\", \"processed_response_locutions\"],\n",
    "[\"processed_question_locution\", \"processed_preceding_locution\", \"processed_response_locutions\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_Y(config, data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    combined_text = data[config[0]].astype(str)\n",
    "    for column in config[1:]:\n",
    "        combined_text += ' ' + data[column].astype(str)\n",
    "    tokenizer.fit_on_texts(combined_text)\n",
    "    X = tokenizer.texts_to_sequences(combined_text.values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    y = pd.get_dummies(data['question_type']).values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix_and_accuracy(Y_predicted, y_test, batch_size, epochs, config):\n",
    "    y_pred = []\n",
    "    for subarr in Y_predicted:\n",
    "        max_index = np.argmax(subarr)\n",
    "        subarr_output = np.zeros_like(subarr)\n",
    "        subarr_output[max_index] = int(1)\n",
    "        y_pred.append(subarr_output)\n",
    "\n",
    "    y_pred = np.array(y_pred).tolist()\n",
    "    y_pred = [[int(num) for num in arr] for arr in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_balanced =  balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    matrix = ConfusionMatrixDisplay(confusion_matrix=cm).plot();\n",
    "    \n",
    "    config = [entry.split()[0] for entry in config]\n",
    "    folder_name = ' '.join(config)\n",
    "    plt.savefig('path/confusion matrices/LSTM-multi/' + folder_name + '/batch' + str(batch_size) + ' epochs' + str(epochs) + '.png')\n",
    "     \n",
    "    return accuracy, accuracy_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('path/QT-Questions-train-over.csv')\n",
    "test_data = pd.read_csv('path/QT-Questions-test.csv')\n",
    "    \n",
    "train_data['processed_preceding_locution'] = train_data['processed_preceding_locution'].apply(lambda x: str(x))\n",
    "train_data['processed_preceding_locution'] = train_data['processed_preceding_locution'].apply(lambda x: re.sub(r'^nan$', ' ', str(x)))\n",
    "\n",
    "test_data['processed_preceding_locution'] = test_data['processed_preceding_locution'].apply(lambda x: str(x))\n",
    "test_data['processed_preceding_locution'] = test_data['processed_preceding_locution'].apply(lambda x: re.sub(r'^nan$', ' ', str(x)))\n",
    "\n",
    "train_data['processed_response_locutions'] = train_data['processed_response_locutions'].apply(lambda x: str(x))\n",
    "train_data['processed_response_locutions'] = train_data['processed_response_locutions'].apply(lambda x: re.sub(r'[\\[\\]\\'\\,\"]', ' ', str(x)))\n",
    "\n",
    "test_data['processed_response_locutions'] = test_data['processed_response_locutions'].apply(lambda x: str(x))\n",
    "test_data['processed_response_locutions'] = test_data['processed_response_locutions'].apply(lambda x: re.sub(r'[\\[\\]\\'\\\",]', ' ', str(x)))\n",
    "    \n",
    "with open('path/results/LSTM-multi.csv', 'w', encoding='UTF8') as file:\n",
    "        \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Configuration', 'Batches', 'Epochs', 'Accuracy', 'Balanced accuracy'])\n",
    "        \n",
    "    for config in CONFIGS:\n",
    "        for batch_size in BATCH_SIZES:\n",
    "            for epochs in EPOCHS:\n",
    "                print('Now testing model with config: ' + str(config) + ', batch_size: ' + str(batch_size) + ', epochs: ' + str(epochs))\n",
    "\n",
    "                X_train, y_train = create_X_and_Y(config, train_data)\n",
    "                X_test, y_test = create_X_and_Y(config, test_data)\n",
    "                    \n",
    "                model = create_model(X_train)\n",
    "                \n",
    "                history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])#, class_weight=dict(enumerate(class_weights)))\n",
    "                Y_predicted = model.predict(X_test)\n",
    "                accuracy, accuracy_balanced = create_confusion_matrix_and_accuracy(Y_predicted, y_test, batch_size, epochs, config)\n",
    "                writer.writerow([config, batch_size, epochs, accuracy, accuracy_balanced])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
